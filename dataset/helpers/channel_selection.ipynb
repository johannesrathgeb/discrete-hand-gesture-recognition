{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import entropy\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pyedflib\n",
    "from helpers.utils import preprocess_data\n",
    "\n",
    "DATASET_DIR = 'raw_data/eeg-motor-movement/'\n",
    "dataset_config = {\n",
    "    \"meta_csv\": os.path.join(DATASET_DIR, \"eeg-motor-movement-metadata.csv\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(nr_of_subj=109, trial_type=1, chunk_data=True, chunks=8, base_folder=DATASET_DIR, sample_rate=160,\n",
    "              samples=640, cpu_format=False, preprocessing=False, hp_freq=0.5, bp_low=2, bp_high=60, notch=False,\n",
    "              hp_filter=False, bp_filter=False, artifact_removal=False, rms_feature=False):\n",
    "    # Get file paths\n",
    "    PATH = base_folder\n",
    "    SUBS = glob(PATH + 'S[0-9]*')\n",
    "    FNAMES = sorted([x[-4:] for x in SUBS])\n",
    "    FNAMES = FNAMES[:nr_of_subj]\n",
    "\n",
    "    # Remove the subjects with incorrectly annotated data that will be omitted from the final dataset\n",
    "    subjects = ['S038', 'S088', 'S089', 'S092', 'S100', 'S104']\n",
    "    try:\n",
    "        for sub in subjects:\n",
    "            FNAMES.remove(sub)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # print(\"Using files:\")\n",
    "    # print(FNAMES)\n",
    "\n",
    "    \"\"\"\n",
    "    @input - label (String)\n",
    "            \n",
    "    Helper method that converts trial labels into integer representations\n",
    "\n",
    "    @output - data (Numpy array); target labels (Numpy array)\n",
    "    \"\"\"\n",
    "\n",
    "    def convert_label_to_int(str):\n",
    "        if str == 'T1':\n",
    "            return 0\n",
    "        if str == 'T2':\n",
    "            return 1\n",
    "        raise Exception(\"Invalid label %s\" % str)\n",
    "\n",
    "    \"\"\"\n",
    "    @input - data (array); number of chunks to divide the list into (int)\n",
    "            \n",
    "    Helper method that divides the input list into a given number of arrays\n",
    "\n",
    "    @output - 2D array of divided input data\n",
    "    \"\"\"\n",
    "\n",
    "    def divide_chunks(data, chunks):\n",
    "        for i in range(0, len(data), chunks):\n",
    "            yield data[i:i + chunks]\n",
    "\n",
    "\n",
    "    executed_trials = '03,07,11'.split(',')\n",
    "    imagined_trials = '04,08,12'.split(',')\n",
    "    both_trials = executed_trials + imagined_trials\n",
    "    samples_per_chunk = int(samples / chunks)\n",
    "\n",
    "\n",
    "    # Determine the type of trials to be used\n",
    "    # if trial_type == RunType.Executed:\n",
    "    #     file_numbers = executed_trials\n",
    "    # elif trial_type == RunType.Imagined:\n",
    "    #     file_numbers = imagined_trials\n",
    "    # elif trial_type == RunType.Combined:\n",
    "    #     file_numbers = both_trials\n",
    "    # else:\n",
    "    #     raise Exception(\"Invalid trial type value %d\" % trial_type)\n",
    "    file_numbers = imagined_trials\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Iterate over different subjects\n",
    "    for subj in FNAMES:\n",
    "\n",
    "        # Load the file names for given subject\n",
    "        fnames = glob(os.path.join(PATH, subj, subj + 'R*.edf'))\n",
    "        fnames = [name for name in fnames if name[-6:-4] in file_numbers]\n",
    "\n",
    "        # Iterate over the trials for each subject\n",
    "        for file_name in fnames:\n",
    "\n",
    "            # Load the file\n",
    "            # print(\"File name \" + file_name)\n",
    "            loaded_file = pyedflib.EdfReader(file_name)\n",
    "            annotations = loaded_file.readAnnotations()\n",
    "            times = annotations[0]\n",
    "            durations = annotations[1]\n",
    "            tasks = annotations[2]\n",
    "\n",
    "            # Load the data signals into a buffer\n",
    "            signals = loaded_file.signals_in_file\n",
    "            # signal_labels = loaded_file.getSignalLabels()\n",
    "            sigbufs = np.zeros((signals, loaded_file.getNSamples()[0]))\n",
    "            for i in np.arange(signals):\n",
    "                sigbufs[i, :] = loaded_file.readSignal(i)\n",
    "\n",
    "            # initialize the result arrays with preferred shapes\n",
    "            if chunk_data and not rms_feature:\n",
    "                trial_data = np.zeros((15, 64, chunks, samples_per_chunk))\n",
    "            elif chunk_data and rms_feature:\n",
    "                trial_data = np.zeros((15, 64, chunks))\n",
    "            else:\n",
    "                trial_data = np.zeros((15, 64, samples))\n",
    "            labels = []\n",
    "\n",
    "            signal_start = 0\n",
    "            k = 0\n",
    "\n",
    "            # Iterate over tasks in the trial run\n",
    "            for i in range(len(times)):\n",
    "                # Collects only the 15 non-rest tasks in each run\n",
    "                if k == 15:\n",
    "                    break\n",
    "\n",
    "                current_duration = durations[i]\n",
    "                signal_end = signal_start + samples\n",
    "\n",
    "                # Skipping tasks where the user was resting\n",
    "                if tasks[i] == 'T0':\n",
    "                    signal_start += int(sample_rate * current_duration)\n",
    "                    continue\n",
    "\n",
    "                # Iterate over each channel\n",
    "                for j in range(len(sigbufs)):\n",
    "                    channel_data = sigbufs[j][signal_start:signal_end]\n",
    "                    if preprocessing:\n",
    "                        channel_data = preprocess_data(channel_data, sample_rate=sample_rate, ac_freq=60,\n",
    "                                                       hp_freq=hp_freq, bp_low=bp_low, bp_high=bp_high, notch=notch,\n",
    "                                                       hp_filter=hp_filter, bp_filter=bp_filter,\n",
    "                                                       artifact_removal=artifact_removal)\n",
    "                    if chunk_data:\n",
    "                        channel_data = list(divide_chunks(channel_data, samples_per_chunk))\n",
    "\n",
    "                    if rms_feature:\n",
    "                        channel_data = np.sqrt(np.mean(np.square(channel_data), axis=1))\n",
    "\n",
    "                    # Add data for the current channel and task to the result\n",
    "                    trial_data[k][j] = channel_data\n",
    "\n",
    "                # add label(s) for the current task to the result\n",
    "                if chunk_data:\n",
    "                    # multiply the labels by the chunk size for chunked mode\n",
    "                    labels.extend([convert_label_to_int(tasks[i])] * chunks)\n",
    "                else:\n",
    "                    labels.append(convert_label_to_int(tasks[i]))\n",
    "\n",
    "                signal_start += int(sample_rate * current_duration)\n",
    "                k += 1\n",
    "\n",
    "            # Add labels and data for the current run into the final output numpy arrays\n",
    "            y.extend(labels)\n",
    "            if cpu_format:\n",
    "                if chunk_data:\n",
    "                    # (15, 64, 8, 80) => (15, 64, 80, 8) => (15, 8, 80, 64) => (120, 80, 64)\n",
    "                    X.extend(trial_data.swapaxes(2, 3).swapaxes(1, 3).reshape((-1, samples_per_chunk, 64)))\n",
    "                else:\n",
    "                    # (15, 64, 640) => (15, 640, 64)\n",
    "                    X.extend(trial_data.swapaxes(1, 2))\n",
    "            else:\n",
    "                if chunk_data and not rms_feature:\n",
    "                    # (15, 64, 8, 80) => (15, 8, 64, 80) => (120, 64, 80)\n",
    "                    X.extend(trial_data.swapaxes(1, 2).reshape((-1, 64, samples_per_chunk)))\n",
    "                elif chunk_data and rms_feature:\n",
    "                    # (15, 64, 8) => (15, 8, 64) => (120, 64)\n",
    "                    X.extend(trial_data.swapaxes(1, 2).reshape((-1, 64)))\n",
    "                else:\n",
    "                    # (15, 64, 640)\n",
    "                    X.extend(trial_data)\n",
    "\n",
    "    # Shape the final output arrays to the correct format\n",
    "    X = np.stack(X)\n",
    "    y = np.array(y).reshape((-1, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "class SequenceDataset(TorchDataset):\n",
    "    def __init__(self, X, y, rms_feature=False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.rms_feature = rms_feature\n",
    "        self.window_samples = 80 # 640/8\n",
    "        self.window_samples = 4\n",
    "\n",
    "    def get_windows_rms(self, eeg_data):\n",
    "        # calculate number of windows for data length\n",
    "        #num_samples = eeg_data.shape[1]\n",
    "\n",
    "\n",
    "        # num_windows = (num_samples - self.window_samples) // self.step_samples + 1\n",
    "        num_windows = 160\n",
    "        # num_windows = 20\n",
    "        rms_windows = []    \n",
    "        for i in range(num_windows):\n",
    "            # calculate start and end point for window\n",
    "            start = i * self.window_samples\n",
    "            end = start + self.window_samples\n",
    "            \n",
    "            # zero-pad window if it's longer than remaining data\n",
    "            # print(start, end)\n",
    "            \n",
    "            segment = eeg_data[:, start:end]\n",
    "            # make sure window has correct length\n",
    "            # segment = segment[:, :self.window_samples]\n",
    "            # Compute RMS for each channel in the window\n",
    "            # print(segment.shape)\n",
    "            rms_feature = np.sqrt(np.mean(np.square(segment.cpu().numpy()), axis=1))  \n",
    "            rms_windows.append(rms_feature)\n",
    "        windows_np = np.array(rms_windows) \n",
    "        \n",
    "        # return windows tensor of consistent shape and true number of windows (without padding) \n",
    "        return torch.tensor(windows_np, dtype=torch.float32), num_windows\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_data = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        # Make sure y is a single integer (no extra dim).\n",
    "        y_data = torch.tensor(self.y[idx], dtype=torch.long).squeeze()\n",
    "\n",
    "        # x_data = x_data.T\n",
    "        selected_channels = [10, 17, 18, 11, 16, 9, 50, 51]\n",
    "        if self.rms_feature:\n",
    "            x_data, length = self.get_windows_rms(x_data)\n",
    "            # if selected_channels:\n",
    "            #     x_data = x_data[:, selected_channels]\n",
    "            x_data = x_data.T    \n",
    "        else:\n",
    "            length = x_data.shape[1]\n",
    "            # if selected_channels:\n",
    "            #     x_data = x_data[selected_channels, :]\n",
    "\n",
    "        # print(\"shape\", x_data.shape)\n",
    "\n",
    "        return x_data, length, y_data\n",
    "\n",
    "def get_train_val_test_split(train_ratio=0.8, val_ratio=0.1, shuffle=True, random_seed=42, rms_feature=False):\n",
    "    \"\"\"\n",
    "    Split arrays (X and y) into random train, val, test subsets.\n",
    "\n",
    "    Args:\n",
    "        X: numpy array or list-like of shape (N, ...)\n",
    "        y: numpy array or list-like of shape (N,)\n",
    "        train_ratio: float between (0, 1)\n",
    "        val_ratio: float between (0, 1)\n",
    "        shuffle: bool, whether to shuffle data before splitting\n",
    "        random_seed: int or None, for reproducible output across multiple calls\n",
    "\n",
    "    Returns:\n",
    "        (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = load_data(nr_of_subj=109, chunk_data=(not rms_feature), chunks=8, cpu_format=False,\n",
    "                 preprocessing=(not rms_feature), hp_freq=0.5, bp_low=2, bp_high=60, notch=True,\n",
    "                 hp_filter=False, bp_filter=True, artifact_removal=True)\n",
    "\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    # print(\"Start\")\n",
    "    N = len(X)\n",
    "    \n",
    "    indices = np.arange(N)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    # print(\"Get ends\")\n",
    "    train_end = int(N * train_ratio)\n",
    "    val_end   = int(N * (train_ratio + val_ratio))\n",
    "    # print(\"Indices\")\n",
    "    train_idx = indices[:train_end]\n",
    "    val_idx   = indices[train_end:val_end]\n",
    "    test_idx  = indices[val_end:]\n",
    "    \n",
    "    # print(\"Splits\")\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_val,   y_val   = X[val_idx],   y[val_idx]\n",
    "    X_test,  y_test  = X[test_idx],  y[test_idx]\n",
    "    # print(\"Datasets\")\n",
    "    train_dataset = SequenceDataset(X_train, y_train, rms_feature)\n",
    "    val_dataset   = SequenceDataset(X_val,   y_val, rms_feature)\n",
    "    test_dataset  = SequenceDataset(X_test,  y_test, rms_feature)\n",
    "    full_dataset  = SequenceDataset(X, y, rms_feature)\n",
    "    # print(\"Done\")\n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_train_val_test_split(rms_feature=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_channel_entropy(data_loader):\n",
    "    channel_entropies = []\n",
    "    total_trials = 0\n",
    "    \n",
    "    # Iterate through the DataLoader\n",
    "    for windows, original_length, label in data_loader:\n",
    "        for trial in windows:  # Iterate over trials in the batch\n",
    "            trial_entropies = []\n",
    "            for channel_data in trial:  # Iterate over channels in the trial\n",
    "                # Compute entropy for the channel\n",
    "                hist, _ = np.histogram(channel_data.numpy(), bins=50, density=True)\n",
    "                channel_entropy = entropy(hist)\n",
    "                trial_entropies.append(channel_entropy)\n",
    "            \n",
    "            # Accumulate trial entropies\n",
    "            if len(channel_entropies) == 0:\n",
    "                channel_entropies = np.array(trial_entropies)\n",
    "            else:\n",
    "                channel_entropies += np.array(trial_entropies)\n",
    "            total_trials += 1\n",
    "    \n",
    "    # Average the entropy values across all trials\n",
    "    average_entropies = channel_entropies / total_trials\n",
    "    return average_entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dl = DataLoader(dataset=ds,\n",
    "                         batch_size=64)\n",
    "average_entropies = compute_channel_entropy(full_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.38508116 3.39829385 3.40315092 3.40862522 3.40595538 3.40300463\n",
      " 3.39557294 3.38514179 3.39563531 3.40290204 3.40627457 3.40520445\n",
      " 3.40234082 3.39494077 3.38023233 3.39366868 3.3988124  3.40299252\n",
      " 3.40333562 3.40110929 3.39640055 3.38378504 3.38695644 3.38784327\n",
      " 3.38180831 3.38911086 3.40137265 3.39417319 3.38831157 3.38081684\n",
      " 3.38735194 3.39765467 3.40260854 3.4050226  3.40385471 3.40173076\n",
      " 3.39372801 3.38981061 3.38024442 3.39106833 3.37158099 3.38911584\n",
      " 3.35392882 3.39477189 3.36488476 3.38785306 3.36088145 3.37502396\n",
      " 3.38768698 3.39382957 3.39892653 3.39970062 3.39765516 3.39266575\n",
      " 3.38526637 3.36339748 3.38065944 3.3927856  3.39213817 3.38229721\n",
      " 3.3639379  3.36636424 3.37393801 3.31378357]\n"
     ]
    }
   ],
   "source": [
    "print(average_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 8 Channels (Indices): [ 3 10  4 11 33 34 18  2]\n"
     ]
    }
   ],
   "source": [
    "channel_indices = np.argsort(average_entropies)[::-1]  # Descending order\n",
    "top_8_channels = channel_indices[:8]  # Select top 8\n",
    "print(\"Top 8 Channels (Indices):\", top_8_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HAND-RECOGNITION",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
