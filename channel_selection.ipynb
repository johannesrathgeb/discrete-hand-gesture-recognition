{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import mne\n",
    "\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.signal import iirnotch, butter, filtfilt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGMotorMovementDataset(TorchDataset):\n",
    "    \"\"\"\n",
    "    Basic EMG-EPN612 Dataset: loads data from files\n",
    "    \"\"\"\n",
    "    #max-length 664 before\n",
    "    def __init__(self, grouped_df, fs=160, window_size=0.25, overlap=0.0, max_samples=656, min_samples=476, window_mode=\"rms\", scaling=True):\n",
    "        \"\"\"\n",
    "        @param grouped_df: dataset df grouped by File Path\n",
    "        @param fs: frequency sample rate in Hz\n",
    "        @param window_size: length of window in ms\n",
    "        @param overlap: overlap of windows in ms\n",
    "        @param max_samples: max sample length in dataset (599 for EMG-EPN612)\n",
    "        \"\"\"  \n",
    "        self.fs = fs\n",
    "        self.window_mode = window_mode\n",
    "        self.window_samples = int(window_size * fs)\n",
    "        self.step_samples = int(self.window_samples * (1 - overlap))\n",
    "        self.max_samples = max_samples\n",
    "        self.min_samples = min_samples\n",
    "        self.max_windows = (self.max_samples - self.window_samples) // self.step_samples + 1\n",
    "        self.scaling = scaling\n",
    "\n",
    "        print(\"max samples\", self.max_samples)\n",
    "        print(\"max windows\", self.max_windows)\n",
    "        labels = []\n",
    "        locations = []\n",
    "        start_indices = []\n",
    "        end_indices = []\n",
    "\n",
    "        all_data = []\n",
    "        for folder_path, group in grouped_df:\n",
    "            for label in group[\"Label\"].unique():\n",
    "                label_group = group[group[\"Label\"] == label]\n",
    "                # sampled = label_group.sample(num_reps, replace=False, random_state=42)\n",
    "                # Extend the lists with the sampled data\n",
    "                labels.extend(label_group[\"Label\"].values)\n",
    "                locations.extend(label_group[\"File_Path\"].values)\n",
    "                start_indices.extend(label_group[\"Start_Index\"].values)\n",
    "                end_indices.extend(label_group[\"End_Index\"].values)\n",
    "                if scaling:\n",
    "                    for _, row in label_group.iterrows():\n",
    "                        file_path = row[\"File_Path\"]\n",
    "                        start_idx = row[\"Start_Index\"]\n",
    "                        end_idx = row[\"End_Index\"]\n",
    "                        eeg_data = self.get_eeg_data(file_path, start_idx, end_idx, str(file_path) + '.csv', True)\n",
    "                        # print(\"EEG DATA SHAPE\")\n",
    "                        # print(eeg_data.shape) \n",
    "                        all_data.append(eeg_data)\n",
    "        if scaling:\n",
    "            print(\"ALL DATA SHAPE\")\n",
    "            print(len(all_data)) \n",
    "            all_data = np.stack(all_data)  \n",
    "            print(all_data.shape) #(num items, num channels, samples)\n",
    "            reshaped_x = all_data.reshape(all_data.shape[0], all_data.shape[1] * all_data.shape[2])\n",
    "            print(reshaped_x.shape)\n",
    "            self.scaler = MinMaxScaler()\n",
    "            self.scaler.fit(reshaped_x)  \n",
    "            print(f\"Scaler fitted: Min={self.scaler.data_min_}, Max={self.scaler.data_max_}\")\n",
    "            del all_data\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        locations = np.array(locations)\n",
    "        start_indices = np.array(start_indices)\n",
    "        end_indices = np.array(end_indices)\n",
    "\n",
    "        # Encode labels into integer values, print unique counts\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        labels = torch.from_numpy(le.fit_transform(labels.reshape(-1)))\n",
    "        self.print_unique_labels(labels)   \n",
    "\n",
    "        # save all data into flat array for later use\n",
    "        self.all_gestures = self.create_flat_array(labels, locations, start_indices, end_indices)\n",
    "        print(self.all_gestures.shape)\n",
    "\n",
    "\n",
    "    def print_unique_labels(self, labels):\n",
    "        unique_labels, counts = torch.unique(labels, return_counts=True)\n",
    "        print(\"Unique labels and counts:\")\n",
    "        for label, count in zip(unique_labels.tolist(), counts.tolist()):\n",
    "            print(f\"Label {label}: {count} occurrences\")\n",
    "\n",
    "    def create_flat_array(self, labels, locations, start_indices, end_indices):\n",
    "        flat_array = []\n",
    "        label_idx = 0\n",
    "        for file_path in locations:\n",
    "            flat_array.append([file_path, labels[label_idx], start_indices[label_idx], end_indices[label_idx]])\n",
    "            label_idx += 1\n",
    "        return np.array(flat_array, dtype=object)    \n",
    "\n",
    "    def get_eeg_data(self, file_path, start_idx, end_idx, output_file, padding=False):\n",
    "        # convert file path to h5 instead of json\n",
    "        raw = mne.io.read_raw_edf(file_path, preload=False, verbose=False)\n",
    "        raw_data_segment, times = raw[:, start_idx:end_idx]\n",
    "\n",
    "        \n",
    "        #eeg_data = self.normalize_and_group_channels(raw_data_segment, raw.info['ch_names'])\n",
    "        eeg_data = np.array(raw_data_segment)\n",
    "        # print(raw.info['ch_names'])\n",
    "        #eeg_data = self.combine_channels(eeg_data, raw.info['ch_names'])\n",
    "        # print(eeg_data.shape)\n",
    "        # print(eeg_data.dtype)\n",
    "        #eeg_data = eeg_data.T\n",
    "        #print(eeg_data.shape)\n",
    "        # header = \",\".join([f\"Channel{i+1}\" for i in range(eeg_data.shape[0])])\n",
    "        # np.savetxt(output_file, eeg_data, delimiter=',', header=header, comments='')\n",
    "        def apply_notch_filter(data, freq, fs, quality_factor=30):\n",
    "            notch_freq = freq / (fs / 2)  # Normalized frequency\n",
    "            b, a = iirnotch(notch_freq, quality_factor)\n",
    "            return lfilter(b, a, data, axis=1)\n",
    "\n",
    "        # Define Butterworth band-pass filter\n",
    "        def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "            nyquist = 0.5 * fs\n",
    "            low = lowcut / nyquist\n",
    "            high = highcut / nyquist\n",
    "            b, a = butter(order, [low, high], btype='band')\n",
    "            return lfilter(b, a, data, axis=1)\n",
    "\n",
    "        eeg_data = apply_notch_filter(eeg_data, freq=60, fs=160)\n",
    "\n",
    "        # Apply Butterworth band-pass filter\n",
    "        eeg_data = butter_bandpass_filter(eeg_data, lowcut=2, highcut=60, fs=160)\n",
    "\n",
    "\n",
    "        if padding:\n",
    "            num_samples = eeg_data.shape[1]\n",
    "            pad_size = self.max_samples - num_samples\n",
    "            if pad_size > 0:\n",
    "                padding = np.zeros((eeg_data.shape[0], pad_size))\n",
    "                eeg_data = np.hstack((eeg_data, padding))\n",
    "            eeg_data = eeg_data[:, :self.max_samples]\n",
    "        return eeg_data \n",
    "    \n",
    "    def get_high_pass_filtered_data(self, emg_data):\n",
    "        num_samples = emg_data.shape[1]\n",
    "        pad_size = self.max_samples - num_samples\n",
    "        if pad_size > 0:\n",
    "            padding = np.zeros((emg_data.shape[0], pad_size))\n",
    "            emg_data = np.hstack((emg_data, padding))\n",
    "        # emg_data = self.normalize_eeg(emg_data, local_min, local_max, -100.0, 100.0)\n",
    "        # Ensure emg_data has contiguous memory layout\n",
    "        emg_data = emg_data.copy()\n",
    "        # for channel_idx in range(emg_data.shape[0]):\n",
    "        #     emg_data[channel_idx] = self.highpass_filter(\n",
    "        #         emg_data[channel_idx], 20, self.fs, 5\n",
    "        #     )\n",
    "        # return windows tensor of consistent shape and true number of windows (without padding) \n",
    "        # print(emg_data.shape)\n",
    "        return torch.tensor(emg_data, dtype=torch.float32), num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Step 1: Fetch file path, gesture index, label and location\n",
    "        file_path, label, start_idx, end_idx = self.all_gestures[index]\n",
    "        # Step 2: Load EMG data from h5 file\n",
    "        emg_data = self.get_eeg_data(file_path, start_idx, end_idx, str(index) + '.csv', True)\n",
    "        # if self.scaling:\n",
    "        #     eeg_data_scaled_flat = self.scaler.transform(emg_data.reshape(1, -1))\n",
    "        #     # print(eeg_data_scaled_flat.shape)\n",
    "        #     eeg_data_scaled = eeg_data_scaled_flat.reshape(emg_data.shape)  # Shape: (channels, samples)\n",
    "        #     emg_data = eeg_data_scaled\n",
    "        # Step 3: Create RMS windows and get original length\n",
    "\n",
    "\n",
    "        # print(\"highpass\")\n",
    "        windows, original_length = self.get_high_pass_filtered_data(emg_data)\n",
    "\n",
    "\n",
    "        return windows, original_length, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_gestures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max samples 656\n",
      "max windows 16\n",
      "ALL DATA SHAPE\n",
      "6489\n",
      "(6489, 64, 656)\n",
      "(6489, 41984)\n",
      "Scaler fitted: Min=[-9.36448330e-05 -3.11927451e-04 -3.30637775e-04 ... -1.58537296e-04\n",
      " -1.33784116e-04 -1.99436679e-04], Max=[0.00012621 0.00041959 0.00043925 ... 0.00019207 0.00021472 0.00024047]\n",
      "Unique labels and counts:\n",
      "Label 0: 2163 occurrences\n",
      "Label 1: 2163 occurrences\n",
      "Label 2: 2163 occurrences\n",
      "(6489, 4)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DATASET_DIR = 'raw_data/eeg-motor-movement/'\n",
    "dataset_config = {\n",
    "    \"meta_csv\": os.path.join(DATASET_DIR, \"eeg-motor-movement-metadata.csv\"),\n",
    "}\n",
    "\n",
    "meta_csv = dataset_config['meta_csv']\n",
    "df = pd.read_csv(meta_csv)\n",
    "df['Folder_Path'] = df['File_Path'].apply(lambda x: os.path.dirname(x))\n",
    "grouped = df.groupby('Folder_Path')\n",
    "group_keys = list(grouped.groups.keys())  # Get the group keys (folder paths)\n",
    "\n",
    "ds_testing = EEGMotorMovementDataset(grouped)\n",
    "\n",
    "test_dl = DataLoader(dataset=ds_testing,\n",
    "                         batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def compute_channel_entropy(data_loader):\n",
    "    channel_entropies = []\n",
    "    total_trials = 0\n",
    "    \n",
    "    # Iterate through the DataLoader\n",
    "    for windows, original_length, label in data_loader:\n",
    "        # Assuming `windows` has shape (batch_size, channels, samples)\n",
    "        print(windows.shape)\n",
    "        for trial in windows:  # Iterate over trials in the batch\n",
    "            trial_entropies = []\n",
    "            for channel_data in trial:  # Iterate over channels in the trial\n",
    "                # Compute entropy for the channel\n",
    "                hist, _ = np.histogram(channel_data.numpy(), bins=50, density=True)\n",
    "                channel_entropy = entropy(hist)\n",
    "                trial_entropies.append(channel_entropy)\n",
    "            \n",
    "            # Accumulate trial entropies\n",
    "            if len(channel_entropies) == 0:\n",
    "                channel_entropies = np.array(trial_entropies)\n",
    "            else:\n",
    "                channel_entropies += np.array(trial_entropies)\n",
    "            total_trials += 1\n",
    "    \n",
    "    # Average the entropy values across all trials\n",
    "    average_entropies = channel_entropies / total_trials\n",
    "    return average_entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([32, 64, 656])\n",
      "torch.Size([25, 64, 656])\n"
     ]
    }
   ],
   "source": [
    "average_entropies = compute_channel_entropy(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 8 Channels (Indices): [10 17 18 11 16  9 50 51]\n"
     ]
    }
   ],
   "source": [
    "channel_indices = np.argsort(average_entropies)[::-1]  # Descending order\n",
    "top_8_channels = channel_indices[:8]  # Select top 8\n",
    "print(\"Top 8 Channels (Indices):\", top_8_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[17 14 19 50 12 15 20 52] --> scaled, rest included, fist runs\n",
    "[51 52 18 20 50 58 17 53] --> unscaled, rest included, fist runs\n",
    "[17 50 14 57 13  8 53 63] --> scaled, rest excluded, fist runs\n",
    "[18 20 52 51 17 50 58 53] --> unscaled, rest excluded, fist runs\n",
    "[53 63 11 20 16 54 50 13] --> scaled, rest excluded, imagined runs\n",
    "[18 20 17 52 51 19 16 53] --> unscaled, rest excluded, imagined runs\n",
    "[16 17 50 63 15 13 11 12] --> scaled, rest included, imagined runs\n",
    "[41 63 13 40 51 54 61 14] --> scaled, rest excludedm real runs\n",
    "[52 51 20 18 50 58 53 57] --> unscaled, rest excluded, real runs\n",
    "[13 54 17 52 19 41  0 51] --> scaled, rest included, real runs\n",
    "\n",
    "\n",
    "[ 7  9 26 23 62 17 21 43] --> scaled, rest included, imagined runs, filtered\n",
    "[10 17 18 11 16  9 50 51] --> unscaled, rest included, imagined runs, filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.36381565, 3.38061149, 3.39254282, 3.40340247, 3.393695  ,\n",
       "       3.38290881, 3.36813049, 3.38165874, 3.39606927, 3.40766729,\n",
       "       3.41598811, 3.41025329, 3.40360521, 3.38955349, 3.38283575,\n",
       "       3.39965336, 3.40877822, 3.41181552, 3.41134504, 3.40272002,\n",
       "       3.3979638 , 3.2116247 , 3.20088124, 3.20579235, 3.24310857,\n",
       "       3.27684753, 3.3051548 , 3.28284909, 3.24451821, 3.29922404,\n",
       "       3.32200044, 3.34409029, 3.35282105, 3.36277782, 3.34605726,\n",
       "       3.34038397, 3.31804241, 3.28901222, 3.35298151, 3.35222523,\n",
       "       3.36760301, 3.38404772, 3.32696348, 3.34909866, 3.35757745,\n",
       "       3.38550132, 3.34955128, 3.37439484, 3.3891997 , 3.39881043,\n",
       "       3.40595092, 3.40492887, 3.40171006, 3.39132499, 3.37721402,\n",
       "       3.35582865, 3.37796192, 3.39540177, 3.39131519, 3.37176698,\n",
       "       3.35024617, 3.35338831, 3.36084565, 3.28205678])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_entropies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HAND-RECOGNITION",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
